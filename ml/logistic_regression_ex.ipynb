{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c16ec9-834d-4803-b680-af1e17ccdf97",
   "metadata": {},
   "source": [
    "# Exercises - Machine Learning for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f2623dd-e9de-425e-990d-78f343d1a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8718a-4fdc-40a1-ad1a-b194f80f13cb",
   "metadata": {},
   "source": [
    "## Sonar dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24025203-47c6-4562-a7aa-56b564f33329",
   "metadata": {},
   "source": [
    "In this exercise, you will solve a binary classification problem solved using logistic regression. The dataset consists of 60 features corresponding to sonar measurements, with a binary label that indicates whether the sample is a rock (0) or a mine (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f1dc6b4-102d-47f3-ba7f-e3fe78d9e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fe4b9e6-ede5-46c1-9804-8697bd2eb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"data/logistic_regression/sonar.csv\", delimiter = \",\")\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e4bb20e-ab2f-4542-9a2a-8e50619785a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling of the dataset\n",
    "np.random.shuffle(data)\n",
    "num_features = X.shape[1]\n",
    "X_shuffled = data[:,:num_features]\n",
    "y_shuffled = data[:,num_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f992de6-8f21-4200-bbcb-6fac7a24704a",
   "metadata": {},
   "source": [
    "1. Standardize the shuffled dataset according to the mean and the standard deviation over the samples. Split the dataset into training (80%) and test (20%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11589a2e-75a0-4dfc-94f9-035f10d3797a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8bf1ae9-56cc-4cde-a021-a9d2d3551e77",
   "metadata": {},
   "source": [
    "2. Use the _LogisticRegression_ class from the example (toy dataset) to build and train a model with _linear decision boundary_. Adjust the number of epochs and learning rate. Plot the cost vs epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fec7be-785c-4740-a9d0-a3e67488d085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c48d1f32-75eb-4732-95b4-b21dcf6f2849",
   "metadata": {},
   "source": [
    "3. Evaluate the misclassifications (number of wrong classifications) over the training and the test sets. Compute the accuracy (number of correct classifications/number of samples) over the training and the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cbb362-c53a-4c4b-9616-caadd0d896b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bc448ac-ef2e-4bd9-bd0a-13676966a8e2",
   "metadata": {},
   "source": [
    "## XOR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64aee47-cfea-4328-9d04-db79c4ad0e1c",
   "metadata": {},
   "source": [
    "In this exercise, you will implement regularized logistic regression to learn the XOR function from a noisy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f8fe995-2395-475c-8d33-cff2611fa6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fce51672-c129-4383-96a6-6450a6067fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and shuffle the dataset\n",
    "data = np.loadtxt(\"data/logistic_regression/xor.txt\", delimiter = \",\")\n",
    "np.random.shuffle(data)\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "num_samples = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b0f6514-bf80-4245-a2b5-e145e710c1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def mapFeatureNormalized(X1, X2):\n",
    "    degree = 8\n",
    "    out = np.ones((len(X1),1))\n",
    "    for i in range(1,degree+1):\n",
    "        for j in range(0,i+1):\n",
    "            prod = (X1**(i-j))*(X2**j)\n",
    "            mean_prod = np.mean(prod)\n",
    "            std_prod = np.std(prod)\n",
    "            norm_prod = (prod-mean_prod)/std_prod\n",
    "            out = np.append(out, norm_prod.reshape(-1,1), axis=1)\n",
    "    return out\n",
    "\n",
    "def plotDecisionBoundary(model):\n",
    "    x1s = np.linspace(np.min(X[:,0]),np.max(X[:,0]),100)\n",
    "    x2s = np.linspace(np.min(X[:,1]),np.max(X[:,1]),100)\n",
    "    xs, ys = np.meshgrid(x1s, x2s, indexing='ij')\n",
    "    Xs = mapFeatureNormalized(xs.ravel(), ys.ravel())\n",
    "    zs = Xs @ model.weights\n",
    "    plt.contour(xs, ys, zs.reshape((len(x1s),len(x2s))), [0.])\n",
    "    plt.scatter(X[:,0], X[:,1], c=y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a2beddf-4911-4946-a342-e08cf6a85cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend the features of the dataset up to sixth powers (with standardization)\n",
    "X_ = mapFeatureNormalized(X[:,0], X[:,1])\n",
    "num_features = X_.shape[1]\n",
    "\n",
    "# Split the dataset (0.6-0.2-0.2)\n",
    "n_train = int(0.6*num_samples)\n",
    "n_val = int(0.2*num_samples)\n",
    "\n",
    "X_train = X_[:n_train,:]\n",
    "y_train = y[:n_train]\n",
    "X_val = X_[n_train:n_train+n_val,:]\n",
    "y_val = y[n_train:n_train+n_val]\n",
    "X_test = X_[n_train+n_val:,:]\n",
    "y_test = y[n_train+n_val:]\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_val = y_val.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e07a43-8078-4fb3-bf3b-81aadf728be2",
   "metadata": {},
   "source": [
    "1. Plot the dataset (scatter plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90416a93-e1db-4a76-b0a1-a8ea88f652f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a0339a9-21c9-4e64-b999-24d2b0b46bc3",
   "metadata": {},
   "source": [
    "2. Implement the _LogisticRegression_ class with Tikhonov regularization. Add a method called `accuracy` that computes the prediction accuracy on a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7c26d7-595c-44d8-b634-4406b52bb903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93fa8464-3b77-45e7-89b8-33c9d34cf231",
   "metadata": {},
   "source": [
    "3. Train a Logistic Regression model on the training set for 10000 epochs with `reg_param = 0.` and `learning_rate = 1.` and evaluate its accuracy on the training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c292580-367a-4df4-8c6c-7570792f18b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "839a97e9-fa97-477c-937c-6771c754d915",
   "metadata": {},
   "source": [
    "4. Plot the cost (training) vs epochs and the decision boundary (use the auxiliary function defined above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08d2a6-1bc9-4d8f-9ce7-da92dacc1512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cb19c53-734b-4707-9929-4530dae0edd9",
   "metadata": {},
   "source": [
    "5. Perform a grid search to find the \"best\" regularization parameter in the interval (0.,1.), while keeping fixed the learning rate and the number of epochs. Re-train the model using the best value and the training+validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cbfcc5-7d5f-4838-ab3d-146fb1b540bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58e65f95-1f55-4f94-a414-5fca23f07316",
   "metadata": {},
   "source": [
    "6. Plot the decision boundary of the fitted model and evaluate the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed4ab93-19c4-4884-8240-900b6ac5781f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
